{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss in PyTorch and its relation with Softmax \n",
    "\n",
    "# [Link to my Youtube Video Explaining this whole Notebook](https://www.youtube.com/watch?v=h3M3Ob0zgkc&list=PLxqBkZuBynVRMORlFw95iNTp9aZzmmz4Y&index=7)\n",
    "\n",
    "[![Imgur](https://imgur.com/r2Qhgo3.png)](https://www.youtube.com/watch?v=h3M3Ob0zgkc&list=PLxqBkZuBynVRMORlFw95iNTp9aZzmmz4Y&index=7)\n",
    "\n",
    "\n",
    "### Cross-entropy is a function that compares two probability distributions.\n",
    "\n",
    "![](assets/2022-02-21-19-07-53.png)\n",
    "\n",
    "The key thing to pay attention to is that cross-entropy is a function that takes, as input, two probability distributions: q and p and returns a value that is minimal when q and p are equal. q represents an estimated distribution, and p represents a true distribution.\n",
    "\n",
    "In the context of ML classification we know the actual label of the training data, so the true/target distribution, p, has a probability of 1 for the true label and 0 elsewhere, i.e. p is a one-hot vector.\n",
    "\n",
    "------------------\n",
    "\n",
    "#### Softmax is a non-linear activation function\n",
    "\n",
    "![](assets/2022-02-21-19-08-58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is not a loss function. It has a very specific task: It is used for multi-class classification to normalize the scores for the given classes. By doing so we get probabilities for each class that sum up to 1.\n",
    "\n",
    "### In PyTorch Softmax is combined with Cross-Entropy-Loss to calculate the `nn.CrossEntropyLoss` of a model.\n",
    "\n",
    "\n",
    "The documentation of [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) says,\n",
    "\n",
    "#### This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "\n",
    "#### So DO NOT use `nn.Softmax()` Function in the output layer for a neural net when using `nn.CrossEntropyLoss` as a loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7055,  2.2556,  0.4626, -0.1980,  1.0355],\n",
       "        [-0.7134, -1.1858,  0.6833,  0.3093, -1.6893],\n",
       "        [-0.8832,  0.5251,  1.1047,  1.4685, -0.9776]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.randn(3, 5, requires_grad=True)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss_pytorch = nn.CrossEntropyLoss()\n",
    "\n",
    "targets = torch.argmax(output, dim=1)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6895, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pytorch = ce_loss_pytorch(output, targets)\n",
    "loss_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7055,  2.2556,  0.4626, -0.1980,  1.0355],\n",
       "        [-0.7134, -1.1858,  0.6833,  0.3093, -1.6893],\n",
       "        [-0.8832,  0.5251,  1.1047,  1.4685, -0.9776]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0324, 0.6252, 0.1041, 0.0538, 0.1846],\n",
       "        [0.1133, 0.0707, 0.4581, 0.3152, 0.0427],\n",
       "        [0.0420, 0.1718, 0.3067, 0.4413, 0.0382]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_proba = torch.softmax(output, axis=1)\n",
    "softmax_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now cross-entropy loss is nothing but a combination of softmax and negative log likelihood loss. Hence, your loss can simply be computed by taking the average of the negative log of the probabilities of your true labels.\n",
    "\n",
    "![](assets/2022-02-21-19-16-14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7533, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_manual = (torch.log(1/softmax_proba[0,3]) +  torch.log(1/softmax_proba[1,3]) + torch.log(1/softmax_proba[2,2])) / 3\n",
    "\n",
    "loss_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_from_tensor :  torch.return_types.max(\n",
      "values=tensor([0.6252, 0.4581, 0.4413], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([1, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "max_from_tensor = torch.max(softmax_proba, dim=1)\n",
    "\n",
    "print('max_from_tensor : ', max_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1_max =  max_from_tensor[0][0]\n",
    "row_2_max =  max_from_tensor[0][1]\n",
    "row_3_max =  max_from_tensor[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6895, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_manual = (torch.log(1/row_1_max) +  torch.log(1/row_2_max) + torch.log(1/row_3_max)) / 3\n",
    "loss_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So overall, we can see that the calculation of Cross Entropy Loss using `nn.CrossEntropyLoss()` and using the manual step by step process with Softmax() gave the same result.\n",
    "\n",
    "#### Hence DO NOT use `nn.Softmax()` Function in the output layer for a neural net when using `nn.CrossEntropyLoss` as a loss function as Pytorch's `nn.CrossEntropyLoss` already combines the softmax within it."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
